{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Check if GPU is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary class to handle mapping between words and numerical indices\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        # Initialize dictionaries for word to index and index to word mappings\n",
    "        self.word2index = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2}\n",
    "        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\"}\n",
    "        self.word_count = {}  # Keep track of word frequencies\n",
    "        self.n_words = 3  # Start counting from 3 to account for special tokens\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        # Add all words in a sentence to the vocabulary\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # Add a word to the vocabulary\n",
    "        if word not in self.word2index:\n",
    "            # Assign a new index to the word and update mappings\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.word_count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            # Increment word count if the word already exists in the vocabulary\n",
    "            self.word_count[word] += 1\n",
    "\n",
    "def tokenize_and_pad(sentences, vocab):\n",
    "    # Calculate the maximum sentence length for padding\n",
    "    max_length = max(len(sentence.split(' ')) for sentence in sentences) + 2  # +2 for SOS and EOS tokens\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Convert each sentence to a list of indices, adding SOS and EOS tokens\n",
    "        tokens = [vocab.word2index[\"<SOS>\"]] + [vocab.word2index[word] for word in sentence.split(' ')] + [vocab.word2index[\"<EOS>\"]]\n",
    "        # Pad sentences to the maximum length\n",
    "        padded_tokens = tokens + [vocab.word2index[\"<PAD>\"]] * (max_length - len(tokens))\n",
    "        tokenized_sentences.append(padded_tokens)\n",
    "    return torch.tensor(tokenized_sentences, dtype=torch.long)\n",
    "\n",
    "# Custom Dataset class for English to French sentences\n",
    "class EngFrDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.eng_vocab = Vocabulary()\n",
    "        self.fr_vocab = Vocabulary()\n",
    "        self.pairs = []\n",
    "\n",
    "        # Process each English-French pair\n",
    "        for eng, fr in pairs:\n",
    "            self.eng_vocab.add_sentence(eng)\n",
    "            self.fr_vocab.add_sentence(fr)\n",
    "            self.pairs.append((eng, fr))\n",
    "\n",
    "        # Separate English and French sentences\n",
    "        self.eng_sentences = [pair[0] for pair in self.pairs]\n",
    "        self.fr_sentences = [pair[1] for pair in self.pairs]\n",
    "        \n",
    "        # Tokenize and pad sentences\n",
    "        self.eng_tokens = tokenize_and_pad(self.eng_sentences, self.eng_vocab)\n",
    "        self.fr_tokens = tokenize_and_pad(self.fr_sentences, self.fr_vocab)\n",
    "\n",
    "        # Define the embedding layers for English and French\n",
    "        self.eng_embedding = torch.nn.Embedding(self.eng_vocab.n_words, 100)  # Embedding size = 100\n",
    "        self.fr_embedding = torch.nn.Embedding(self.fr_vocab.n_words, 100)    # Embedding size = 100\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of sentence pairs\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the tokenized and padded sentences by index\n",
    "        eng_tokens = self.eng_tokens[idx]\n",
    "        fr_tokens = self.fr_tokens[idx]\n",
    "        # Lookup embeddings for the tokenized sentences\n",
    "        eng_emb = self.eng_embedding(eng_tokens)\n",
    "        fr_emb = self.fr_embedding(fr_tokens)\n",
    "        return eng_tokens, fr_tokens, eng_emb, fr_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample dataset of English-French sentence pairs\n",
    "english_to_french = [\n",
    "    (\"i am cold\", \"J'ai froid\"),\n",
    "    (\"you are tired\", \"Tu es fatigué\"),\n",
    "    (\"he is hungry\", \"Il a faim\"),\n",
    "    (\"she is happy\", \"Elle est heureuse\"),\n",
    "    (\"we are friends\", \"Nous sommes amis\"),\n",
    "    (\"they are students\", \"Ils sont étudiants\"),\n",
    "    (\"the cat is sleeping\", \"Le chat dort\"),\n",
    "    (\"the sun is shining\", \"Le soleil brille\"),\n",
    "    (\"we love music\", \"Nous aimons la musique\"),\n",
    "    (\"she speaks French fluently\", \"Elle parle français couramment\"),\n",
    "    (\"he enjoys reading books\", \"Il aime lire des livres\"),\n",
    "    (\"they play soccer every weekend\", \"Ils jouent au football chaque week-end\"),\n",
    "    (\"the movie starts at 7 PM\", \"Le film commence à 19 heures\"),\n",
    "    (\"she wears a red dress\", \"Elle porte une robe rouge\"),\n",
    "    (\"we cook dinner together\", \"Nous cuisinons le dîner ensemble\"),\n",
    "    (\"he drives a blue car\", \"Il conduit une voiture bleue\"),\n",
    "    (\"they visit museums often\", \"Ils visitent souvent des musées\"),\n",
    "    (\"the restaurant serves delicious food\", \"Le restaurant sert une délicieuse cuisine\"),\n",
    "    (\"she studies mathematics at university\", \"Elle étudie les mathématiques à l'université\"),\n",
    "    (\"we watch movies on Fridays\", \"Nous regardons des films le vendredi\"),\n",
    "    (\"he listens to music while jogging\", \"Il écoute de la musique en faisant du jogging\"),\n",
    "    (\"they travel around the world\", \"Ils voyagent autour du monde\"),\n",
    "    (\"the book is on the table\", \"Le livre est sur la table\"),\n",
    "    (\"she dances gracefully\", \"Elle danse avec grâce\"),\n",
    "    (\"we celebrate birthdays with cake\", \"Nous célébrons les anniversaires avec un gâteau\"),\n",
    "    (\"he works hard every day\", \"Il travaille dur tous les jours\"),\n",
    "    (\"they speak different languages\", \"Ils parlent différentes langues\"),\n",
    "    (\"the flowers bloom in spring\", \"Les fleurs fleurissent au printemps\"),\n",
    "    (\"she writes poetry in her free time\", \"Elle écrit de la poésie pendant son temps libre\"),\n",
    "    (\"we learn something new every day\", \"Nous apprenons quelque chose de nouveau chaque jour\"),\n",
    "    (\"the dog barks loudly\", \"Le chien aboie bruyamment\"),\n",
    "    (\"he sings beautifully\", \"Il chante magnifiquement\"),\n",
    "    (\"they swim in the pool\", \"Ils nagent dans la piscine\"),\n",
    "    (\"the birds chirp in the morning\", \"Les oiseaux gazouillent le matin\"),\n",
    "    (\"she teaches English at school\", \"Elle enseigne l'anglais à l'école\"),\n",
    "    (\"we eat breakfast together\", \"Nous prenons le petit déjeuner ensemble\"),\n",
    "    (\"he paints landscapes\", \"Il peint des paysages\"),\n",
    "    (\"they laugh at the joke\", \"Ils rient de la blague\"),\n",
    "    (\"the clock ticks loudly\", \"L'horloge tic-tac bruyamment\"),\n",
    "    (\"she runs in the park\", \"Elle court dans le parc\"),\n",
    "    (\"we travel by train\", \"Nous voyageons en train\"),\n",
    "    (\"he writes a letter\", \"Il écrit une lettre\"),\n",
    "    (\"they read books at the library\", \"Ils lisent des livres à la bibliothèque\"),\n",
    "    (\"the baby cries\", \"Le bébé pleure\"),\n",
    "    (\"she studies hard for exams\", \"Elle étudie dur pour les examens\"),\n",
    "    (\"we plant flowers in the garden\", \"Nous plantons des fleurs dans le jardin\"),\n",
    "    (\"he fixes the car\", \"Il répare la voiture\"),\n",
    "    (\"they drink coffee in the morning\", \"Ils boivent du café le matin\"),\n",
    "    (\"the sun sets in the evening\", \"Le soleil se couche le soir\"),\n",
    "    (\"she dances at the party\", \"Elle danse à la fête\"),\n",
    "    (\"we play music at the concert\", \"Nous jouons de la musique au concert\"),\n",
    "    (\"he cooks dinner for his family\", \"Il cuisine le dîner pour sa famille\"),\n",
    "    (\"they study French grammar\", \"Ils étudient la grammaire française\"),\n",
    "    (\"the rain falls gently\", \"La pluie tombe doucement\"),\n",
    "    (\"she sings a song\", \"Elle chante une chanson\"),\n",
    "    (\"we watch a movie together\", \"Nous regardons un film ensemble\"),\n",
    "    (\"he sleeps deeply\", \"Il dort profondément\"),\n",
    "    (\"they travel to Paris\", \"Ils voyagent à Paris\"),\n",
    "    (\"the children play in the park\", \"Les enfants jouent dans le parc\"),\n",
    "    (\"the walks along the beach\", \"Elle se promène le long de la plage\"),\n",
    "    (\"we talk on the phone\", \"Nous parlons au téléphone\"),\n",
    "    (\"He waits for the bus\", \"Il attend le bus\"),\n",
    "    (\"They visit the Eiffel Tower\", \"Ils visitent la tour Eiffel\"),\n",
    "    (\"The stars twinkle at night\", \"Les étoiles scintillent la nuit\"),\n",
    "    (\"She dreams of flying\", \"Elle rêve de voler\"),\n",
    "    (\"We work in the office\", \"Nous travaillons au bureau\"),\n",
    "    (\"He studies history\", \"Il étudie l'histoire\"),\n",
    "    (\"They listen to the radio\", \"Ils écoutent la radio\"),\n",
    "    (\"The wind blows gently\", \"Le vent souffle doucement\"),\n",
    "    (\"She swims in the ocean\", \"Elle nage dans l'océan\"),\n",
    "    (\"We dance at the wedding\", \"Nous dansons au mariage\"),\n",
    "    (\"He climbs the mountain\", \"Il gravit la montagne\"),\n",
    "    (\"They hike in the forest\", \"Ils font de la randonnée dans la forêt\"),\n",
    "    (\"The cat meows loudly\", \"Le chat miaule bruyamment\"),\n",
    "    (\"She paints a picture\", \"Elle peint un tableau\"),\n",
    "    (\"We build a sandcastle\", \"Nous construisons un château de sable\"),\n",
    "    (\"He sings in the choir\", \"Il chante dans le chœur\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sample dataset of english-french sentence pairs\n",
    "english_to_french = [\n",
    "    (\"i am cold\", \"j'ai froid\"),\n",
    "    (\"you are tired\", \"tu es fatigué\"),\n",
    "    (\"he is hungry\", \"il a faim\"),\n",
    "    (\"she is happy\", \"elle est heureuse\"),\n",
    "    (\"we are friends\", \"nous sommes amis\"),\n",
    "    (\"they are students\", \"ils sont étudiants\"),\n",
    "    (\"the cat is sleeping\", \"le chat dort\"),\n",
    "    (\"the sun is shining\", \"le soleil brille\"),\n",
    "    (\"we love music\", \"nous aimons la musique\"),\n",
    "    (\"she speaks french fluently\", \"elle parle français couramment\"),\n",
    "    (\"he enjoys reading books\", \"il aime lire des livres\"),\n",
    "    (\"they play soccer every weekend\", \"ils jouent au football chaque week-end\"),\n",
    "    (\"the movie starts at 7 pm\", \"le film commence à 19 heures\"),\n",
    "    (\"she wears a red dress\", \"elle porte une robe rouge\"),\n",
    "    (\"we cook dinner together\", \"nous cuisinons le dîner ensemble\"),\n",
    "    (\"he drives a blue car\", \"il conduit une voiture bleue\"),\n",
    "    (\"they visit museums often\", \"ils visitent souvent des musées\"),\n",
    "    (\"the restaurant serves delicious food\", \"le restaurant sert une délicieuse cuisine\"),\n",
    "    (\"she studies mathematics at university\", \"elle étudie les mathématiques à l'université\"),\n",
    "    (\"we watch movies on fridays\", \"nous regardons des films le vendredi\"),\n",
    "    (\"he listens to music while jogging\", \"il écoute de la musique en faisant du jogging\"),\n",
    "    (\"they travel around the world\", \"ils voyagent autour du monde\"),\n",
    "    (\"the book is on the table\", \"le livre est sur la table\"),\n",
    "    (\"she dances gracefully\", \"elle danse avec grâce\"),\n",
    "    (\"we celebrate birthdays with cake\", \"nous célébrons les anniversaires avec un gâteau\"),\n",
    "    (\"he works hard every day\", \"il travaille dur tous les jours\"),\n",
    "    (\"they speak different languages\", \"ils parlent différentes langues\"),\n",
    "    (\"the flowers bloom in spring\", \"les fleurs fleurissent au printemps\"),\n",
    "    (\"she writes poetry in her free time\", \"elle écrit de la poésie pendant son temps libre\"),\n",
    "    (\"we learn something new every day\", \"nous apprenons quelque chose de nouveau chaque jour\"),\n",
    "    (\"the dog barks loudly\", \"le chien aboie bruyamment\"),\n",
    "    (\"he sings beautifully\", \"il chante magnifiquement\"),\n",
    "    (\"they swim in the pool\", \"ils nagent dans la piscine\"),\n",
    "    (\"the birds chirp in the morning\", \"les oiseaux gazouillent le matin\"),\n",
    "    (\"she teaches english at school\", \"elle enseigne l'anglais à l'école\"),\n",
    "    (\"we eat breakfast together\", \"nous prenons le petit déjeuner ensemble\"),\n",
    "    (\"he paints landscapes\", \"il peint des paysages\"),\n",
    "    (\"they laugh at the joke\", \"ils rient de la blague\"),\n",
    "    (\"the clock ticks loudly\", \"l'horloge tic-tac bruyamment\"),\n",
    "    (\"she runs in the park\", \"elle court dans le parc\"),\n",
    "    (\"we travel by train\", \"nous voyageons en train\"),\n",
    "    (\"he writes a letter\", \"il écrit une lettre\"),\n",
    "    (\"they read books at the library\", \"ils lisent des livres à la bibliothèque\"),\n",
    "    (\"the baby cries\", \"le bébé pleure\"),\n",
    "    (\"she studies hard for exams\", \"elle étudie dur pour les examens\"),\n",
    "    (\"we plant flowers in the garden\", \"nous plantons des fleurs dans le jardin\"),\n",
    "    (\"he fixes the car\", \"il répare la voiture\"),\n",
    "    (\"they drink coffee in the morning\", \"ils boivent du café le matin\"),\n",
    "    (\"the sun sets in the evening\", \"le soleil se couche le soir\"),\n",
    "    (\"she dances at the party\", \"elle danse à la fête\"),\n",
    "    (\"we play music at the concert\", \"nous jouons de la musique au concert\"),\n",
    "    (\"he cooks dinner for his family\", \"il cuisine le dîner pour sa famille\"),\n",
    "    (\"they study french grammar\", \"ils étudient la grammaire française\"),\n",
    "    (\"the rain falls gently\", \"la pluie tombe doucement\"),\n",
    "    (\"she sings a song\", \"elle chante une chanson\"),\n",
    "    (\"we watch a movie together\", \"nous regardons un film ensemble\"),\n",
    "    (\"he sleeps deeply\", \"il dort profondément\"),\n",
    "    (\"they travel to paris\", \"ils voyagent à paris\"),\n",
    "    (\"the children play in the park\", \"les enfants jouent dans le parc\"),\n",
    "    (\"the walks along the beach\", \"elle se promène le long de la plage\"),\n",
    "    (\"we talk on the phone\", \"nous parlons au téléphone\"),\n",
    "    (\"he waits for the bus\", \"il attend le bus\"),\n",
    "    (\"they visit the eiffel tower\", \"ils visitent la tour eiffel\"),\n",
    "    (\"the stars twinkle at night\", \"les étoiles scintillent la nuit\"),\n",
    "    (\"she dreams of flying\", \"elle rêve de voler\"),\n",
    "    (\"we work in the office\", \"nous travaillons au bureau\"),\n",
    "    (\"he studies history\", \"il étudie l'histoire\"),\n",
    "    (\"they listen to the radio\", \"ils écoutent la radio\"),\n",
    "    (\"the wind blows gently\", \"le vent souffle doucement\"),\n",
    "    (\"she swims in the ocean\", \"elle nage dans l'océan\"),\n",
    "    (\"we dance at the wedding\", \"nous dansons au mariage\"),\n",
    "    (\"he climbs the mountain\", \"il gravit la montagne\"),\n",
    "    (\"they hike in the forest\", \"ils font de la randonnée dans la forêt\"),\n",
    "    (\"the cat meows loudly\", \"le chat miaule bruyamment\"),\n",
    "    (\"she paints a picture\", \"elle peint un tableau\"),\n",
    "    (\"we build a sandcastle\", \"nous construisons un château de sable\"),\n",
    "    (\"he sings in the choir\", \"il chante dans le chœur\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and test sets\n",
    "english_to_french_train, english_to_french_test = train_test_split(english_to_french, test_size=0.3, random_state=42)\n",
    "BATCH_SIZE = 24\n",
    "\n",
    "# Initialize training dataset and DataLoader\n",
    "train_dataset = EngFrDataset(english_to_french_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Initialize test dataset and DataLoader\n",
    "test_dataset = EngFrDataset(english_to_french_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize full dataset\n",
    "engFrDataset = EngFrDataset(english_to_french)\n",
    "train_loader= DataLoader(engFrDataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Positional Encoding\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model, max_len=5000):\n",
    "#         super(PositionalEncoding, self).__init__()\n",
    "#         self.encoding = torch.zeros(max_len, d_model).to(device)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1).to(device)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model)).to(device)\n",
    "#         self.encoding[:, 0::2] = torch.sin(position * div_term).to(device)\n",
    "#         self.encoding[:, 1::2] = torch.cos(position * div_term).to(device)\n",
    "#         self.encoding = self.encoding.unsqueeze(0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return x + self.encoding[:, :x.size(1)].detach()\n",
    "\n",
    "# # Define the Transformer model\n",
    "# class Transformer(nn.Module):\n",
    "#     def __init__(self, input_dim, output_dim, d_model=100, nhead=2, num_encoder_layers=4, num_decoder_layers=4):\n",
    "#         super(Transformer, self).__init__()\n",
    "        \n",
    "#         self.pos_encoder = PositionalEncoding(d_model)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(\n",
    "#             nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead),\n",
    "#             num_layers=num_encoder_layers\n",
    "#         )\n",
    "#         self.transformer_decoder = nn.TransformerDecoder(\n",
    "#             nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead),\n",
    "#             num_layers=num_decoder_layers\n",
    "#         )\n",
    "        \n",
    "#         self.linear = nn.Linear(d_model, output_dim)\n",
    "\n",
    "#     def forward(self, src, tgt):\n",
    "#         src = self.pos_encoder(src.permute(1, 0, 2))  # Change the dimension for Transformer input\n",
    "#         tgt = self.pos_encoder(tgt.permute(1, 0, 2))  # Change the dimension for Transformer input\n",
    "#         # print(f\"Source: {src}\")\n",
    "#         # print(f\"Target: {tgt}\")\n",
    "#         memory = self.transformer_encoder(src)\n",
    "#         output = self.transformer_decoder(tgt, memory)\n",
    "#         output = self.linear(output)\n",
    "#         # print(f\"Output: {output}\")\n",
    "#         return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code pulled from https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1).to(device)\n",
    "        output = torch.matmul(attn_probs, V).to(device)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "    \n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model).to(device)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1).to(device)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float().to(device) * -(math.log(10000.0) / d_model)).to(device)\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term).to(device)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term).to(device)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x\n",
    "    \n",
    "class TransformerProf(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(TransformerProf, self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length).to(device), diagonal=1)).bool().to(device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = engFrDataset.eng_vocab.n_words\n",
    "tgt_vocab_size = engFrDataset.fr_vocab.n_words\n",
    "d_model = 1024\n",
    "num_heads = 4\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 11\n",
    "dropout = 0.15\n",
    "model = TransformerProf(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)#, betas=(0.9, 0.98), eps=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train Loss: 1.2531415224075317, Train Accuracy: 72.168284789644%, Val Loss: 4.244198322296143, Val Accuracy: 26.573426573426573%\n",
      "Epoch 20, Train Loss: 0.0886365423599879, Train Accuracy: 99.67637540453075%, Val Loss: 4.950009346008301, Val Accuracy: 27.27272727272727%\n",
      "Epoch 30, Train Loss: 0.023357717941204708, Train Accuracy: 100.0%, Val Loss: 5.271173477172852, Val Accuracy: 27.972027972027973%\n",
      "Epoch 40, Train Loss: 0.015086292599638304, Train Accuracy: 100.0%, Val Loss: 5.570865631103516, Val Accuracy: 27.972027972027973%\n",
      "Epoch 50, Train Loss: 0.01078661996871233, Train Accuracy: 100.0%, Val Loss: 5.653663158416748, Val Accuracy: 27.972027972027973%\n",
      "Epoch 60, Train Loss: 0.00835720170289278, Train Accuracy: 100.0%, Val Loss: 5.747315406799316, Val Accuracy: 27.972027972027973%\n",
      "Epoch 70, Train Loss: 0.006942074901113908, Train Accuracy: 100.0%, Val Loss: 5.8521199226379395, Val Accuracy: 27.972027972027973%\n",
      "Epoch 80, Train Loss: 0.005420183762907982, Train Accuracy: 100.0%, Val Loss: 5.954805374145508, Val Accuracy: 27.972027972027973%\n",
      "Epoch 90, Train Loss: 0.004601139730463426, Train Accuracy: 100.0%, Val Loss: 6.039309978485107, Val Accuracy: 27.972027972027973%\n",
      "Epoch 100, Train Loss: 0.0038717862529059253, Train Accuracy: 100.0%, Val Loss: 6.102209568023682, Val Accuracy: 27.972027972027973%\n",
      "Epoch 110, Train Loss: 0.0035573462955653667, Train Accuracy: 100.0%, Val Loss: 6.153502941131592, Val Accuracy: 27.972027972027973%\n",
      "Epoch 120, Train Loss: 0.0029643336310982704, Train Accuracy: 100.0%, Val Loss: 6.174468517303467, Val Accuracy: 27.972027972027973%\n",
      "Epoch 130, Train Loss: 0.0025713203164438405, Train Accuracy: 100.0%, Val Loss: 6.255016326904297, Val Accuracy: 27.972027972027973%\n",
      "Epoch 140, Train Loss: 0.0024356701566527286, Train Accuracy: 100.0%, Val Loss: 6.317600727081299, Val Accuracy: 27.972027972027973%\n",
      "Epoch 150, Train Loss: 0.0021706264621267715, Train Accuracy: 100.0%, Val Loss: 6.366369724273682, Val Accuracy: 27.972027972027973%\n",
      "Epoch 160, Train Loss: 0.0019220728039120634, Train Accuracy: 100.0%, Val Loss: 6.370034694671631, Val Accuracy: 27.972027972027973%\n",
      "Epoch 170, Train Loss: 0.0017608078972746928, Train Accuracy: 100.0%, Val Loss: 6.451906681060791, Val Accuracy: 27.972027972027973%\n",
      "Epoch 180, Train Loss: 0.001548764140655597, Train Accuracy: 100.0%, Val Loss: 6.481278896331787, Val Accuracy: 27.972027972027973%\n",
      "Epoch 190, Train Loss: 0.0014912715026487906, Train Accuracy: 100.0%, Val Loss: 6.541245937347412, Val Accuracy: 27.972027972027973%\n",
      "Epoch 200, Train Loss: 0.0013900089931363861, Train Accuracy: 100.0%, Val Loss: 6.5577712059021, Val Accuracy: 27.972027972027973%\n",
      "Epoch 210, Train Loss: 0.0012897007012118895, Train Accuracy: 100.0%, Val Loss: 6.617244720458984, Val Accuracy: 27.972027972027973%\n",
      "Epoch 220, Train Loss: 0.001217732671648264, Train Accuracy: 100.0%, Val Loss: 6.648680210113525, Val Accuracy: 27.972027972027973%\n",
      "Epoch 230, Train Loss: 0.0010971168521791697, Train Accuracy: 100.0%, Val Loss: 6.611714839935303, Val Accuracy: 27.972027972027973%\n",
      "Epoch 240, Train Loss: 0.0010456082333500187, Train Accuracy: 100.0%, Val Loss: 6.692707538604736, Val Accuracy: 27.972027972027973%\n",
      "Epoch 250, Train Loss: 0.00101646373514086, Train Accuracy: 100.0%, Val Loss: 6.679492950439453, Val Accuracy: 27.972027972027973%\n",
      "Epoch 260, Train Loss: 0.0009135372238233685, Train Accuracy: 100.0%, Val Loss: 6.745301723480225, Val Accuracy: 27.972027972027973%\n",
      "Epoch 270, Train Loss: 0.0008375088218599558, Train Accuracy: 100.0%, Val Loss: 6.747239112854004, Val Accuracy: 27.972027972027973%\n",
      "Epoch 280, Train Loss: 0.0008568224535944561, Train Accuracy: 100.0%, Val Loss: 6.798666954040527, Val Accuracy: 27.972027972027973%\n",
      "Epoch 290, Train Loss: 0.0007366532032998899, Train Accuracy: 100.0%, Val Loss: 6.830292701721191, Val Accuracy: 27.972027972027973%\n",
      "Epoch 300, Train Loss: 0.0007396793225780129, Train Accuracy: 100.0%, Val Loss: 6.841243267059326, Val Accuracy: 27.972027972027973%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 300\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for eng_tokens, fr_tokens, eng_emb, fr_emb in train_dataloader:\n",
    "        eng_tokens, fr_tokens = eng_tokens.to(device), fr_tokens.to(device)\n",
    "        eng_emb, fr_emb = eng_emb.to(device), fr_emb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(eng_tokens, fr_tokens[:, :-1]) \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        fr_tokens_target = fr_tokens[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(output, fr_tokens_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate correct predictions\n",
    "        predicted = output.argmax(dim=1)\n",
    "        correct = (predicted == fr_tokens_target).sum().item()\n",
    "\n",
    "        # Number of non-padding tokens\n",
    "        non_pad_tokens = (fr_tokens_target != 0).sum().item()\n",
    "        \n",
    "        # Accumulate correct predictions and total tokens (excluding padding tokens)\n",
    "        total_correct += correct\n",
    "        total_tokens += non_pad_tokens\n",
    "    \n",
    "    # Calculate training accuracy and loss\n",
    "    train_accuracy = (total_correct / total_tokens) * 100\n",
    "    train_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    if ((epoch+1) % 10 == 0):\n",
    "        # Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        total_val_loss = 0\n",
    "        total_val_correct = 0\n",
    "        total_val_tokens = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for eng_tokens, fr_tokens, eng_emb, fr_emb in test_dataloader:\n",
    "                eng_tokens, fr_tokens = eng_tokens.to(device), fr_tokens.to(device)\n",
    "                eng_emb, fr_emb = eng_emb.to(device), fr_emb.to(device)\n",
    "                output = model(eng_tokens, fr_tokens[:, :-1])\n",
    "                output_dim = output.shape[-1]\n",
    "                output = output.contiguous().view(-1, output_dim)\n",
    "                fr_tokens_target = fr_tokens[:, 1:].contiguous().view(-1)\n",
    "                loss = criterion(output, fr_tokens_target)\n",
    "\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # Calculate correct predictions\n",
    "                predicted = output.argmax(dim=1)\n",
    "                correct = (predicted == fr_tokens_target).sum().item()\n",
    "\n",
    "                # Number of non-padding tokens\n",
    "                non_pad_tokens = (fr_tokens_target != 0).sum().item()\n",
    "\n",
    "                # Accumulate correct predictions and total tokens (excluding padding tokens)\n",
    "                total_val_correct += correct\n",
    "                total_val_tokens += non_pad_tokens\n",
    "\n",
    "        # Calculate validation accuracy and loss\n",
    "        val_accuracy = (total_val_correct / total_val_tokens) * 100\n",
    "        val_loss = total_val_loss / len(test_dataloader)\n",
    "\n",
    "        # Print training and validation metrics\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss}, Train Accuracy: {train_accuracy}%, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: ['we', 'play', 'music', 'at', 'the', 'concert', '<EOS>']\n",
      "Target Sentence: ['nous', 'jouons', 'de', 'la', 'musique', 'au', 'concert', '<EOS>']\n",
      "Predicted French Sentence: ['nous', 'de', 'musique', 'pluie', 'au', 'lire']\n"
     ]
    }
   ],
   "source": [
    "# Training loop (unchanged)\n",
    "\n",
    "# After the training loop\n",
    "# Perform inference on a validation sentence\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    # Choose a random validation sentence\n",
    "    idx = random.randint(0, len(test_dataset) - 1)\n",
    "    eng_tokens, fr_tokens, eng_emb, fr_emb = test_dataset[idx]\n",
    "    eng_tokens, fr_tokens = eng_tokens.unsqueeze(0).to(device), fr_tokens.unsqueeze(0).to(device)\n",
    "    eng_emb, fr_emb = eng_emb.unsqueeze(0).to(device), fr_emb.unsqueeze(0).to(device)\n",
    "\n",
    "    # Run inference\n",
    "    output = model(eng_tokens, fr_tokens[:, :-1])\n",
    "    output_dim = output.shape[-1]\n",
    "    output = output.view(-1, output_dim)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "    # Convert target indices to French words\n",
    "    input_sentence = [test_dataset.eng_vocab.index2word[idx.item()] for idx in eng_tokens.squeeze(0)]\n",
    "    # Remove padding and SOS token\n",
    "    input_sentence = [word for word in input_sentence if word not in [\"<PAD>\", '<SOS>']]\n",
    "\n",
    "    # Convert predicted indices to French words\n",
    "    predicted_sentence = [test_dataset.fr_vocab.index2word[idx.item()] for idx in predicted]\n",
    "    # Remove padding and EOS token\n",
    "    predicted_sentence = [word for word in predicted_sentence if word not in [\"<PAD>\", \"<EOS>\"]]\n",
    "\n",
    "    # Convert target indices to French words\n",
    "    target_sentence = [test_dataset.fr_vocab.index2word[idx.item()] for idx in fr_tokens.squeeze(0)]\n",
    "    # Remove padding and SOS token\n",
    "    target_sentence = [word for word in target_sentence if word not in [\"<PAD>\", '<SOS>']]\n",
    "\n",
    "    # Print the original English sentence, target French sentence, and predicted French sentence\n",
    "    print(\"Input Sentence:\", input_sentence)\n",
    "    print(\"Target Sentence:\", target_sentence)\n",
    "    print(\"Predicted French Sentence:\", predicted_sentence)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
