{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "torch.set_printoptions(edgeitems=2, linewidth=75)\n",
    "torch.manual_seed(123)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# dataloading\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "batch_size = 32\n",
    "\n",
    "cifar10_train = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definitions\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "  def __init__(self, n_classes, dropout_chance):\n",
    "    super(AlexNet, self).__init__()\n",
    "\n",
    "    self.conv_layers = nn.Sequential(\n",
    "      # Layer 1\n",
    "      nn.Conv2d (3, 32, kernel_size=3, stride=1, padding=0),\n",
    "      nn.BatchNorm2d(32),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(p=dropout_chance),\n",
    "      nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "      # Layer 2\n",
    "      nn.Conv2d (32, 64, kernel_size=5, stride=1, padding=2),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(p=dropout_chance),\n",
    "      nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "      # Layer 3\n",
    "      nn.Conv2d (64, 128, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(p=dropout_chance),\n",
    "      # Layer 4\n",
    "      nn.Conv2d (128, 128, kernel_size=3, stride=1, padding=1),\n",
    "      nn.ReLU(),\n",
    "      nn.Dropout(p=dropout_chance),\n",
    "      # Layer 5\n",
    "      nn.Conv2d (128, 64, kernel_size=3, stride=1, padding=1),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "      nn.Dropout(p=dropout_chance)\n",
    "    )\n",
    "\n",
    "    self.dense_layers = nn.Sequential(\n",
    "      nn.Flatten(),\n",
    "      # Layer 6\n",
    "      nn.Linear(64*2*2, 128),\n",
    "      nn.ReLU(),\n",
    "      # Layer 7\n",
    "      nn.Linear(128, 128),\n",
    "      nn.ReLU(),\n",
    "      # Layer 8\n",
    "      nn.Linear(128, n_classes),\n",
    "      nn.LogSoftmax(dim=1)\n",
    "    )\n",
    "   \n",
    "  def forward(self, x):\n",
    "    x = self.conv_layers(x)\n",
    "    x = self.dense_layers(x)\n",
    "    return x\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1  # Expansion factor to adjust the number of output channels if needed\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, dropout_chance, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        # First convolutional layer of the block\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)  # Batch normalization after the first convolution\n",
    "        self.drop = nn.Dropout(p=dropout_chance) if dropout_chance > 0 else None\n",
    "        \n",
    "        # Second convolutional layer of the block\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)  # Batch normalization after the second convolution\n",
    "\n",
    "        # Shortcut connection to match input and output dimensions if necessary\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the first convolution, batch norm, and ReLU activation\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        # Forward pass through the second convolution and batch norm\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        # Adding dropout\n",
    "        out = self.drop(out) if self.drop != None else out\n",
    "        # Adding the shortcut connection's output to the main path's output\n",
    "        out += self.shortcut(x)\n",
    "        # Final ReLU activation after adding the shortcut\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, block, n_blocks, dropout_chance=0.5, n_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64  # Initial number of input channels\n",
    "\n",
    "        # Initial convolutional layer before entering the residual blocks\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)  # Batch normalization after the initial convolution\n",
    "        \n",
    "        # Creating layers of blocks with increasing channel sizes\n",
    "        self.layer1 = self._make_layer(block, 64, n_blocks[0], dropout_chance=dropout_chance, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, n_blocks[1], dropout_chance=dropout_chance, stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, n_blocks[2], dropout_chance=dropout_chance, stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, n_blocks[3], dropout_chance=dropout_chance, stride=2)\n",
    "        \n",
    "        # Final fully connected layer for classification\n",
    "        self.linear = nn.Linear(512 * block.expansion, n_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, n_blocks, dropout_chance, stride):\n",
    "        # Helper function to create a layer with specified blocks\n",
    "        strides = [stride] + [1]*(n_blocks-1)  # First block could have a stride and the rest have stride of 1\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride=stride, dropout_chance=dropout_chance))\n",
    "            self.in_channels = out_channels * block.expansion  # Update in_channels for the next block\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the initial convolution, batch norm, and ReLU activation\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        # Forward pass through all the layers of blocks\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        # Global average pooling before the final layer\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)  # Flatten the output for the fully connected layer\n",
    "        out = self.linear(out)  # Final classification layer\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training/Validation Loops\n",
    "def print_model_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total Parameters: {total_params}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader, val_loader):\n",
    "    # Lists for storing loss values and validation accuracy\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    val_accuracy_list = []\n",
    "    # Training and validation loop\n",
    "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        model.train()  # Set the model to training mode\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        train_loss_list.append(running_loss / len(train_loader))\n",
    "\n",
    "        # Validation loop\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                images, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(images)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        val_loss_list.append(running_loss / len(val_loader))\n",
    "        val_accuracy = 100 * correct / total\n",
    "        val_accuracy_list.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Training loss: {train_loss_list[-1]}, Validation loss: {val_loss_list[-1]}, Validation Accuracy: {val_accuracy}%')\n",
    "    return [train_loss_list, val_loss_list, val_accuracy_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "batch_size = 32\n",
    "train_loader = torch.utils.data.DataLoader(cifar10_train, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "n_classes = 10\n",
    "n_epochs = 20\n",
    "dropout_chance = 0.3\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 398410\n",
      "Trainable Parameters: 398410\n",
      "Epoch 1, Training loss: 1.4405927348426726, Validation loss: 1.2700357324779985, Validation Accuracy: 56.68%\n",
      "Epoch 2, Training loss: 0.9966186016168795, Validation loss: 0.9329486111292062, Validation Accuracy: 67.37%\n",
      "Epoch 3, Training loss: 0.8280734710028289, Validation loss: 0.8452811562976899, Validation Accuracy: 70.58%\n",
      "Epoch 4, Training loss: 0.7305913601673648, Validation loss: 0.7427947720209249, Validation Accuracy: 74.48%\n",
      "Epoch 5, Training loss: 0.6594734474759184, Validation loss: 0.8080914611824024, Validation Accuracy: 71.78%\n",
      "Epoch 6, Training loss: 0.6051733979451221, Validation loss: 0.7226270979014449, Validation Accuracy: 75.38%\n",
      "Epoch 7, Training loss: 0.5525909440207009, Validation loss: 0.6660986446534483, Validation Accuracy: 77.08%\n",
      "Epoch 8, Training loss: 0.5128827611090507, Validation loss: 0.7121517256902049, Validation Accuracy: 75.44%\n",
      "Epoch 9, Training loss: 0.47773542595992696, Validation loss: 0.6449162116922891, Validation Accuracy: 78.36%\n",
      "Epoch 10, Training loss: 0.44911449395420455, Validation loss: 0.6230715283522972, Validation Accuracy: 79.25%\n",
      "Epoch 11, Training loss: 0.41679209056986655, Validation loss: 0.7044189791328991, Validation Accuracy: 76.99%\n",
      "Epoch 12, Training loss: 0.3928098854819171, Validation loss: 0.6689298279083575, Validation Accuracy: 77.64%\n",
      "Epoch 13, Training loss: 0.37084896552661867, Validation loss: 0.687389042573615, Validation Accuracy: 77.97%\n",
      "Epoch 14, Training loss: 0.3495305519734562, Validation loss: 0.7067593136629738, Validation Accuracy: 77.77%\n",
      "Epoch 15, Training loss: 0.3338007518493702, Validation loss: 0.6181618841692281, Validation Accuracy: 80.25%\n",
      "Epoch 16, Training loss: 0.3199025384309539, Validation loss: 0.6910213945677486, Validation Accuracy: 78.17%\n",
      "Epoch 17, Training loss: 0.30627282427639363, Validation loss: 0.7018685102843629, Validation Accuracy: 78.23%\n",
      "Epoch 18, Training loss: 0.28588070604599825, Validation loss: 0.6334530948259579, Validation Accuracy: 80.25%\n",
      "Epoch 19, Training loss: 0.27854961327498184, Validation loss: 0.6980311587786141, Validation Accuracy: 78.23%\n",
      "Epoch 20, Training loss: 0.27283104169000744, Validation loss: 0.6800321318661443, Validation Accuracy: 79.13%\n"
     ]
    }
   ],
   "source": [
    "# Run AlexNet without dropout\n",
    "\n",
    "Net_1 = AlexNet(n_classes=n_classes, dropout_chance=0).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(Net_1.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9) \n",
    "\n",
    "print_model_parameters(Net_1)\n",
    "\n",
    "train_loss_list_1, val_loss_list_1, val_accuracy_list_1 = training_loop(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer,\n",
    "    model = Net_1,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 398410\n",
      "Trainable Parameters: 398410\n",
      "Epoch 1, Training loss: 1.702021241569397, Validation loss: 1.5225956180986886, Validation Accuracy: 50.99%\n",
      "Epoch 2, Training loss: 1.2569625383756593, Validation loss: 1.3798216078608943, Validation Accuracy: 54.79%\n",
      "Epoch 3, Training loss: 1.0989506840705872, Validation loss: 1.1618867826918824, Validation Accuracy: 61.92%\n",
      "Epoch 4, Training loss: 0.9926018026572195, Validation loss: 1.0784214611251515, Validation Accuracy: 63.51%\n",
      "Epoch 5, Training loss: 0.9093193631102012, Validation loss: 1.0324637023404764, Validation Accuracy: 66.15%\n",
      "Epoch 6, Training loss: 0.8564154198172759, Validation loss: 0.9234217677634364, Validation Accuracy: 70.26%\n",
      "Epoch 7, Training loss: 0.8077857723162866, Validation loss: 0.9574294067419375, Validation Accuracy: 68.58%\n",
      "Epoch 8, Training loss: 0.7718592683855571, Validation loss: 0.8180684796727884, Validation Accuracy: 73.93%\n",
      "Epoch 9, Training loss: 0.7456347072307528, Validation loss: 0.8597093195960925, Validation Accuracy: 71.74%\n",
      "Epoch 10, Training loss: 0.7087082205982599, Validation loss: 0.8359422511376512, Validation Accuracy: 72.84%\n",
      "Epoch 11, Training loss: 0.6854571657861873, Validation loss: 0.8490977032116046, Validation Accuracy: 71.38%\n",
      "Epoch 12, Training loss: 0.6666508965380132, Validation loss: 0.8436219049528384, Validation Accuracy: 71.22%\n",
      "Epoch 13, Training loss: 0.6493054783962052, Validation loss: 0.8704946758076787, Validation Accuracy: 70.7%\n",
      "Epoch 14, Training loss: 0.6323793242451325, Validation loss: 0.7535797294717246, Validation Accuracy: 74.6%\n",
      "Epoch 15, Training loss: 0.6212373188746257, Validation loss: 0.8369275817094138, Validation Accuracy: 71.64%\n",
      "Epoch 16, Training loss: 0.6075310549309676, Validation loss: 0.6637899013944327, Validation Accuracy: 78.36%\n",
      "Epoch 17, Training loss: 0.5962284359311112, Validation loss: 0.8157817504276483, Validation Accuracy: 72.48%\n",
      "Epoch 18, Training loss: 0.5948489133795332, Validation loss: 0.7031188155896366, Validation Accuracy: 76.03%\n",
      "Epoch 19, Training loss: 0.5819682850242043, Validation loss: 0.8373629385099625, Validation Accuracy: 71.23%\n",
      "Epoch 20, Training loss: 0.5791158380698334, Validation loss: 0.7561638583771337, Validation Accuracy: 74.44%\n"
     ]
    }
   ],
   "source": [
    "# Run AlexNet with dropout\n",
    "\n",
    "Net_1_drop = AlexNet(n_classes=n_classes, dropout_chance=dropout_chance).to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(Net_1_drop.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9) \n",
    "\n",
    "print_model_parameters(Net_1_drop)\n",
    "\n",
    "train_loss_list_1d, val_loss_list_1d, val_accuracy_list_1d = training_loop(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer,\n",
    "    model = Net_1_drop,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 11173962\n",
      "Trainable Parameters: 11173962\n",
      "Epoch 1, Training loss: 1.693639721919235, Validation loss: 1.443128769009258, Validation Accuracy: 46.94%\n",
      "Epoch 2, Training loss: 1.3197217740390215, Validation loss: 1.2526015875438532, Validation Accuracy: 54.47%\n",
      "Epoch 3, Training loss: 1.1113442737020442, Validation loss: 1.0709847308957159, Validation Accuracy: 61.79%\n",
      "Epoch 4, Training loss: 0.9548424317603377, Validation loss: 0.9620003717395064, Validation Accuracy: 65.49%\n",
      "Epoch 5, Training loss: 0.8171594699109432, Validation loss: 0.9955667239218093, Validation Accuracy: 64.76%\n",
      "Epoch 6, Training loss: 0.6996850139978065, Validation loss: 0.8980688998303094, Validation Accuracy: 68.87%\n",
      "Epoch 7, Training loss: 0.5780116889389829, Validation loss: 0.8843061843047888, Validation Accuracy: 69.63%\n",
      "Epoch 8, Training loss: 0.465583418711057, Validation loss: 0.8447716318951628, Validation Accuracy: 71.48%\n",
      "Epoch 9, Training loss: 0.3549374931814269, Validation loss: 1.010319872881277, Validation Accuracy: 67.88%\n",
      "Epoch 10, Training loss: 0.25241324210912464, Validation loss: 1.2051819139204847, Validation Accuracy: 65.24%\n",
      "Epoch 11, Training loss: 0.17485226753691566, Validation loss: 1.0430085064884953, Validation Accuracy: 69.62%\n",
      "Epoch 12, Training loss: 0.11780046819520355, Validation loss: 1.080466007748351, Validation Accuracy: 70.2%\n",
      "Epoch 13, Training loss: 0.08088809596785772, Validation loss: 1.0282500760433393, Validation Accuracy: 71.07%\n",
      "Epoch 14, Training loss: 0.06157827570614949, Validation loss: 1.5055483078328185, Validation Accuracy: 65.13%\n",
      "Epoch 15, Training loss: 0.047110896371662506, Validation loss: 1.048746559471368, Validation Accuracy: 71.89%\n",
      "Epoch 16, Training loss: 0.035969809413025355, Validation loss: 1.1210559492294019, Validation Accuracy: 71.39%\n",
      "Epoch 17, Training loss: 0.027415212864826077, Validation loss: 1.069947896293177, Validation Accuracy: 72.36%\n",
      "Epoch 18, Training loss: 0.022080652912323716, Validation loss: 1.1182656231493995, Validation Accuracy: 71.87%\n",
      "Epoch 19, Training loss: 0.020340876228796344, Validation loss: 1.111374190773446, Validation Accuracy: 72.16%\n",
      "Epoch 20, Training loss: 0.01736457858572643, Validation loss: 1.2396230755236963, Validation Accuracy: 70.47%\n"
     ]
    }
   ],
   "source": [
    "# Run ResNet18 without dropout\n",
    "\n",
    "Net_2 = ResNet18(BasicBlock, [2, 2, 2, 2], n_classes=10, dropout_chance=0.0).to(device)  # 4 blocks with 1 layer each\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(Net_2.parameters(), lr=learning_rate) \n",
    "\n",
    "print_model_parameters(Net_2)\n",
    "\n",
    "train_loss_list_2, val_loss_list_2, val_accuracy_list_2 = training_loop(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer,\n",
    "    model = Net_2,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 11173962\n",
      "Trainable Parameters: 11173962\n",
      "Epoch 1, Training loss: 1.8701069701274693, Validation loss: 2.395828752471997, Validation Accuracy: 20.94%\n",
      "Epoch 2, Training loss: 1.568544747504529, Validation loss: 2.0324183089283707, Validation Accuracy: 33.06%\n",
      "Epoch 3, Training loss: 1.407243894485808, Validation loss: 1.6495289581652266, Validation Accuracy: 42.59%\n",
      "Epoch 4, Training loss: 1.2855474345591003, Validation loss: 1.61087693916723, Validation Accuracy: 45.47%\n",
      "Epoch 5, Training loss: 1.1844227875720517, Validation loss: 1.525179234937357, Validation Accuracy: 49.33%\n",
      "Epoch 6, Training loss: 1.0915645328532102, Validation loss: 1.302609054425273, Validation Accuracy: 55.68%\n",
      "Epoch 7, Training loss: 1.020651616504081, Validation loss: 1.1464200408313983, Validation Accuracy: 60.77%\n",
      "Epoch 8, Training loss: 0.9600746631050293, Validation loss: 1.0912083560666337, Validation Accuracy: 62.55%\n",
      "Epoch 9, Training loss: 0.9093468193434326, Validation loss: 0.9724494028396119, Validation Accuracy: 66.39%\n",
      "Epoch 10, Training loss: 0.8607517384178579, Validation loss: 1.0011333488046932, Validation Accuracy: 66.02%\n",
      "Epoch 11, Training loss: 0.8217165848801552, Validation loss: 1.044113399312138, Validation Accuracy: 65.52%\n",
      "Epoch 12, Training loss: 0.7802617804064479, Validation loss: 0.8781979401081134, Validation Accuracy: 70.42%\n",
      "Epoch 13, Training loss: 0.7459662193605249, Validation loss: 0.843237371109545, Validation Accuracy: 71.95%\n",
      "Epoch 14, Training loss: 0.7137743364292616, Validation loss: 0.9137916747754374, Validation Accuracy: 69.79%\n",
      "Epoch 15, Training loss: 0.6826777147964568, Validation loss: 0.8512535636036541, Validation Accuracy: 71.68%\n",
      "Epoch 16, Training loss: 0.6587276806720006, Validation loss: 0.8532898380828742, Validation Accuracy: 72.06%\n",
      "Epoch 17, Training loss: 0.6294830501537176, Validation loss: 0.8715891287730525, Validation Accuracy: 71.53%\n",
      "Epoch 18, Training loss: 0.6059590054012154, Validation loss: 0.7192933687958093, Validation Accuracy: 76.31%\n",
      "Epoch 19, Training loss: 0.5855184360070634, Validation loss: 0.644424737166292, Validation Accuracy: 78.47%\n",
      "Epoch 20, Training loss: 0.5597599344858357, Validation loss: 0.6112095234207452, Validation Accuracy: 79.82%\n"
     ]
    }
   ],
   "source": [
    "# Run ResNet18 with dropout\n",
    "\n",
    "Net_2_drop = ResNet18(BasicBlock, [2, 2, 2, 2], n_classes=10, dropout_chance=dropout_chance).to(device)  # 4 blocks with 1 layer each\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(Net_2_drop.parameters(), lr=learning_rate) \n",
    "\n",
    "print_model_parameters(Net_2_drop)\n",
    "\n",
    "train_loss_list_2d, val_loss_list_2d, val_accuracy_list_2d = training_loop(\n",
    "    n_epochs = n_epochs,\n",
    "    optimizer = optimizer,\n",
    "    model = Net_2_drop,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
